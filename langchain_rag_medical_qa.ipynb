{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Question Answering with LangChain and Hugging Face\n",
    "\n",
    "This document outlines a Python code snippet that demonstrates a natural language processing (NLP) workflow for medical question-answering using the LangChain framework and Hugging Face's Transformers library. The code sets up components, defines a data processing pipeline, executes the pipeline, and measures the execution time. It is tailored for the MASHQA dataset, which focuses on medical questions and answers.\n",
    "\n",
    "The document is divided into the following sections:\n",
    "\n",
    "1. **Setting up Components for Medical Question Answering**: In this section, we initialize the essential components for NLP tasks, such as embeddings, tokenizers, and models, specifically designed for medical question-answering.\n",
    "\n",
    "2. **Initializing a Text-to-Text Generation Model**: This section introduces the code responsible for initializing a text-to-text generation model tailored for medical questions and answers.\n",
    "\n",
    "3. **Building a Data Processing Pipeline with RAG Model**: The next section focuses on constructing a data processing pipeline using the LangChain framework and explains the role of each stage within the pipeline for medical question-answering.\n",
    "\n",
    "4. **Executing the Data Processing Pipeline and Measuring Execution Time**: The final section demonstrates the execution of the data processing pipeline with a specific medical question from the MASHQA dataset and measures the time taken for the pipeline to complete.\n",
    "\n",
    "These code snippets and explanations offer a comprehensive overview of how to set up and utilize NLP components for medical question-answering, build a processing pipeline, and measure execution time in the context of the MASHQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/balu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Embed and store splits\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain import hub\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    " # RAG chain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# torch.cuda.set_device('cpu')\n",
    "# dont use cuda\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Components for LangChain Modules\n",
    "\n",
    "This code segment is responsible for setting up the components required for the processing of custom data using LangChain framework, specifically, for retrieval-augmented generation. It performs the following tasks:\n",
    "\n",
    "1. **Embeddings Model**: Defines the model name for embeddings, which is \"alibidaran/medical_transcription_generator\".\n",
    "\n",
    "2. **Initializing Embeddings**: Initializes an embeddings object using the Hugging Face model specified.\n",
    "\n",
    "3. **Vector Store Creation**: Initializes a vector store using the Chroma library, specifying the `persist_directory` and the embedding function.\n",
    "\n",
    "4. **Retriever Creation**: Creates a retriever using the vector store, enabling efficient text data retrieval.\n",
    "\n",
    "5. **Prompt Loading**: Loads a prompt for the Retrieval-Augmented Generation (RAG) model using the `hub.pull` method.\n",
    "\n",
    "These components are essential for processing and generating text data by using vector database based solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/balu/.cache/torch/sentence_transformers/alibidaran_medical_transcription_generator. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# Define the model name for embeddings\n",
    "embeddings_model_name = \"alibidaran/medical_transcription_generator\"\n",
    "\n",
    "# Initialize an embeddings object using the Hugging Face model specified\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "# Initialize a vector store using the Chroma library, specifying the persist_directory and the embedding function\n",
    "vectorstore = Chroma(persist_directory=\"./vector_stores/vectorstore_train/\", embedding_function=embeddings)\n",
    "\n",
    "# Create a retriever using the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Load a prompt for the RAG (Retrieval-Augmented Generation) model using hub.pull\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a Text-to-Text Generation Model\n",
    "\n",
    "In this section, we initialize a text-to-text generation model for natural language processing tasks. The code accomplishes the following:\n",
    "\n",
    "1. **Model Selection**: Specifies the model ID as 'google/flan-t5-small', indicating the choice of the model for text generation.\n",
    "\n",
    "2. **Tokenizer Initialization**: Initializes a tokenizer using the selected model's pretrained weights.\n",
    "\n",
    "3. **Model Initialization**: Initializes the text-to-text generation model using the model ID, with additional parameters such as `load_in_8bit` and `device_map` settings.\n",
    "\n",
    "4. **Pipeline Setup**: Creates a text-to-text generation pipeline that utilizes the model and tokenizer. It sets the maximum generated text length to 128 characters.\n",
    "\n",
    "5. **HuggingFace Pipeline**: Wraps the pipeline in a HuggingFacePipeline object for easier interaction with the model.\n",
    "\n",
    "These steps prepare the model and associated components for text-to-text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model ID for the text-to-text generation model\n",
    "model_id = 'google/flan-t5-small'\n",
    "\n",
    "# Initialize the tokenizer using the selected model's pretrained weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Initialize the text-to-text generation model using the model ID, with additional parameters\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=False, device_map='cpu')\n",
    "\n",
    "# Create a text-to-text generation pipeline using the model and tokenizer, limiting the generated text length\n",
    "pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in a HuggingFacePipeline object for easier interaction with the model\n",
    "hf = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Data Processing Pipeline with RAG Model\n",
    "\n",
    "In this section, we construct a data processing pipeline that involves the use of a Retrieval-Augmented Generation (RAG) model. The code accomplishes the following:\n",
    "\n",
    "1. **Pipeline Definition**: We define a data processing pipeline using the LangChain framework. The pipeline consists of three stages: retrieving relevant information, forming a question, and generating a response.\n",
    "\n",
    "2. **Retriever**: The initial stage of the pipeline involves the `retriever`, which is responsible for retrieving context or information from a dataset.\n",
    "\n",
    "3. **Question Generation**: The second stage utilizes `RunnablePassthrough()` to create a question based on the retrieved context.\n",
    "\n",
    "4. **RAG Model**: The third stage of the pipeline incorporates a Retrieval-Augmented Generation (RAG) model, `rag_prompt`, to generate responses based on the context and question.\n",
    "\n",
    "5. **HuggingFace Pipeline**: We then use the HuggingFace pipeline (`hf`) to execute the complete data processing chain.\n",
    "\n",
    "The result of this pipeline is a generated response based on the provided context and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Define a data processing pipeline with LangChain\n",
    "rag_chain = (\n",
    "    # First stage: Retrieving relevant information\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "\n",
    "    # Second stage: Utilizing RAG model to generate responses\n",
    "    | rag_prompt\n",
    "\n",
    "    # Third stage: Using the HuggingFace pipeline for execution\n",
    "    | hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Data Processing Pipeline and Measuring Execution Time\n",
    "\n",
    "In this section, we execute the data processing pipeline and measure the execution time. The code accomplishes the following:\n",
    "\n",
    "1. **Time Measurement Start**: We record the current time before executing the pipeline to measure the time it takes to complete the processing.\n",
    "\n",
    "2. **Pipeline Execution**: The `rag_chain` is invoked with a specific question, \"What are the symptoms of ischemic heart disease?\".\n",
    "\n",
    "3. **Result Output**: We print the result of the pipeline's execution, which includes the generated response.\n",
    "\n",
    "4. **Time Measurement End**: We calculate the time taken to execute the pipeline and print the elapsed time.\n",
    "\n",
    "This code provides a practical example of using the data processing pipeline with a specific question and measures the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systolic heart failure\n",
      "24.305009603500366\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Record the start time for measuring execution time\n",
    "s = time.time()\n",
    "\n",
    "# Execute the data processing pipeline with a specific question\n",
    "ress = rag_chain.invoke(\"What are the symptoms of ischemic heart disease?\")\n",
    "\n",
    "# Print the result of the pipeline's execution\n",
    "print(ress)\n",
    "\n",
    "# Calculate and print the elapsed time for execution\n",
    "print(time.time()-s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
